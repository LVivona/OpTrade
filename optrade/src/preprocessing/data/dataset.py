import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from typing import Tuple, Iterator
from datetime import datetime, timedelta

from optrade.data.thetadata.stocks import get_stock_data
from optrade.data.thetadata.contracts import Contract
from optrade.src.preprocessing.data.volatility import get_historical_volatility

from typing import List, Tuple, Iterator, Dict, Any, Optional
from datetime import datetime, timedelta
import pickle
import json
import os
from pydantic import BaseModel, Field


from rich.console import Console

class ContractDataset:
    """
    A dataset containing options contracts generated with consistent parameters.

    Contracts are generated by starting from total_start_date and advancing by
    contract_stride days until reaching the last valid date that allows for
    contracts within the specified time-to-expiration tolerance.
    """

    def __init__(
        self,
        root: str = "AAPL",
        total_start_date: str = "20231107",
        total_end_date: str = "20241114",
        contract_stride: int = 5,
        interval_min: int = 1,
        right: str = "C",
        target_tte: int = 30,
        tte_tolerance: Tuple[int, int] = (25, 35),
        moneyness: str = "OTM",
        target_band: float = 0.05,
        volatility_type: str = "period",
        volatility_scaled: bool = True,
        volatility_scalar: float = 1.0,
        volatility_window: float = 0.8,
    ):
        """
        Initialize the ContractDataset with the specified parameters.

        Args:
            root: The security root symbol
            total_start_date: Start date for the dataset (YYYYMMDD)
            total_end_date: End date for the dataset (YYYYMMDD)
            contract_stride: Days between consecutive contracts
            interval_min: Data interval in minutes
            right: Option type (C/P)
            target_tte: Target time to expiration in days
            tte_tolerance: Acceptable range for TTE as (min_days, max_days)
            moneyness: Contract moneyness (OTM/ATM/ITM)
            target_band: Target percentage band for strike selection
            volatility_type: Type of volatility measure to use
            volatility_scaled: Whether to scale by volatility
            volatility_scalar: Scaling factor for volatility
            volatility_window: Window size for volatility calculation
        """
        self.root = root
        self.total_start_date = total_start_date
        self.total_end_date = total_end_date
        self.contract_stride = contract_stride
        self.interval_min = interval_min
        self.right = right
        self.target_tte = target_tte
        self.tte_tolerance = tte_tolerance
        self.moneyness = moneyness
        self.target_band = target_band
        self.volatility_type = volatility_type
        self.volatility_scaled = volatility_scaled
        self.volatility_scalar = volatility_scalar
        self.volatility_window = volatility_window

        self.ctx = Console()
        self.contracts = []
        self._get_hist_vol()
        self._generate_contracts()

    def _get_hist_vol(self):
        # Calculate number of days to use for historical volatility
        total_days = (pd.to_datetime(self.total_end_date, format='%Y%m%d') - pd.to_datetime(self.total_start_date, format='%Y%m%d')).days
        num_vol_days = int(self.volatility_window * total_days)
        vol_end_date = (pd.to_datetime(self.total_start_date, format='%Y%m%d') + pd.Timedelta(days=num_vol_days)).strftime('%Y%m%d')

        stock_data = get_stock_data(
            root=self.root,
            start_date=self.total_start_date,
            end_date=self.total_end_date,
            interval_min=self.interval_min,
            clean_up=True,
        )

        # Select only the first num_vol_days for calculating volatility
        stock_data = stock_data.loc[stock_data['datetime'] <= vol_end_date]

        # Calculate historical volatility
        self.hist_vol = get_historical_volatility(stock_data, self.volatility_type)
        self.ctx.log(f"Historical volatility from {self.total_start_date} to {vol_end_date}: {self.hist_vol}")

    def _generate_contracts(self):
        """
        Generate all contracts in the dataset based on configuration parameters.
        """
        # Parse dates
        start_date = datetime.strptime(self.total_start_date, "%Y%m%d")
        end_date = datetime.strptime(self.total_end_date, "%Y%m%d")
        max_tte = max(self.tte_tolerance)

        # Calculate the latest possible start date
        latest_start = end_date - timedelta(days=max_tte)

        # Generate contracts
        current_date = start_date
        self.ctx.log(f"Current start date: {current_date}")

        while current_date <= latest_start:
            # Format initial date string
            date_str = current_date.strftime("%Y%m%d")
            self.ctx.log(f"Finding optimal contract for {date_str}")
            attempt_date = current_date
            contract = None

            # Find a valid contract for the current date. Some dates may be ineligible due to holidays or weekends.
            while contract is None and attempt_date <= latest_start:
                attempt_date_str = attempt_date.strftime("%Y%m%d")
                try:
                    contract = Contract.find_optimal(
                        root=self.root,
                        start_date=attempt_date_str,
                        interval_min=self.interval_min,
                        right=self.right,
                        target_tte=self.target_tte,
                        tte_tolerance=self.tte_tolerance,
                        moneyness=self.moneyness,
                        target_band=self.target_band,
                        hist_vol=self.hist_vol,
                        volatility_scaled=self.volatility_scaled,
                        volatility_scalar=self.volatility_scalar,
                    )

                    if attempt_date > current_date:
                        self.ctx.log(f"Found valid contract at shifted date: {attempt_date_str}")

                except Exception as e:
                    self.ctx.log(f"Failed to find contract for {attempt_date_str}: {str(e)}")
                    attempt_date += timedelta(days=1)

                    # Check if we've run out of valid dates
                    if attempt_date > latest_start:
                        self.ctx.log(f"Unable to find valid contract starting from {date_str}")
                        break

                    continue

            # If we found a valid contract, add it and advance by stride
            if contract is not None:
                self.contracts.append(contract)
                self.ctx.log(f"Added contract: {contract}")
                current_date = attempt_date + timedelta(days=self.contract_stride)
            else:
                # If no contract was found, advance by one day to try the next period
                current_date += timedelta(days=1)

            self.ctx.log(f"Next start date: {current_date}")

    def __len__(self) -> int:
        """Get the number of contracts in the dataset."""
        return len(self.contracts)

    def __getitem__(self, idx):
        """Get a contract by index."""
        return self.contracts[idx]

    def __iter__(self) -> Iterator:
        """Iterate through contracts."""
        return iter(self.contracts)

    # def save(self, filepath: str, format: str = 'pickle') -> None:
    #     """
    #     Save the dataset to a file.

    #     Args:
    #         filepath: Path where the dataset will be saved
    #         format: File format ('pickle' or 'json')
    #     """
    #     if format.lower() == 'pickle':
    #         with open(filepath, 'wb') as f:
    #             pickle.dump(self, f)
    #     elif format.lower() == 'json':
    #         # Convert contracts to dictionaries
    #         contract_dicts = [
    #             contract.dict() if hasattr(contract, 'dict') else contract.__dict__
    #             for contract in self.contracts
    #         ]

    #         # Create serializable representation
    #         data = {
    #             'parameters': {
    #                 'root': self.root,
    #                 'total_start_date': self.total_start_date,
    #                 'total_end_date': self.total_end_date,
    #                 'contract_stride': self.contract_stride,
    #                 'interval_min': self.interval_min,
    #                 'right': self.right,
    #                 'target_tte': self.target_tte,
    #                 'tte_tolerance': self.tte_tolerance,
    #                 'moneyness': self.moneyness,
    #                 'target_band': self.target_band,
    #                 'volatility_type': self.volatility_type,
    #                 'volatility_scaled': self.volatility_scaled,
    #                 'volatility_scalar': self.volatility_scalar,
    #                 'volatility_window': self.volatility_window,
    #             },
    #             'contracts': contract_dicts
    #         }

    #         with open(filepath, 'w') as f:
    #             json.dump(data, f, indent=2)
    #     else:
    #         raise ValueError(f"Unsupported format: {format}. Use 'pickle' or 'json'.")

    # @classmethod
    # def load(cls, filepath: str, format: Optional[str] = None) -> 'ContractDataset':
    #     """
    #     Load a dataset from a file.

    #     Args:
    #         filepath: Path to the saved dataset
    #         format: File format ('pickle' or 'json'). If None, determined from file extension.

    #     Returns:
    #         Loaded ContractDataset instance
    #     """
    #     # Determine format from file extension if not specified
    #     if format is None:
    #         _, ext = os.path.splitext(filepath)
    #         if ext.lower() in ['.pkl', '.pickle']:
    #             format = 'pickle'
    #         elif ext.lower() == '.json':
    #             format = 'json'
    #         else:
    #             raise ValueError(f"Cannot determine format from extension '{ext}'. "
    #                              f"Please specify format='pickle' or format='json'.")

    #     if format.lower() == 'pickle':
    #         with open(filepath, 'rb') as f:
    #             return pickle.load(f)
    #     elif format.lower() == 'json':
    #         with open(filepath, 'r') as f:
    #             data = json.load(f)

    #         # Create instance with saved parameters
    #         instance = cls(**data['parameters'])

    #         # If Contract is a Pydantic model
    #         try:
    #             from pydantic import parse_obj_as
    #             instance.contracts = [
    #                 Contract(**contract_dict) for contract_dict in data['contracts']
    #             ]
    #         except (ImportError, TypeError):
    #             # Fallback if not using Pydantic or if Contract init is different
    #             instance.contracts = []
    #             for contract_dict in data['contracts']:
    #                 contract = Contract.find_optimal(
    #                     root=contract_dict.get('root', instance.root),
    #                     start_date=contract_dict.get('start_date'),
    #                     interval_min=contract_dict.get('interval_min', instance.interval_min),
    #                     right=contract_dict.get('right', instance.right),
    #                     target_tte=instance.target_tte,
    #                     tte_tolerance=instance.tte_tolerance,
    #                     moneyness=instance.moneyness,
    #                     target_band=instance.target_band,
    #                     volatility_scaled=instance.volatility_scaled,
    #                     volatility_scalar=instance.volatility_scalar,
    #                     hist_vol=instance.hist_vol,
    #                 )
    #                 # Override with saved values for exp and strike
    #                 contract.exp = contract_dict.get('exp')
    #                 contract.strike = contract_dict.get('strike')
    #                 instance.contracts.append(contract)

    #         return instance
    #     else:
    #         raise ValueError(f"Unsupported format: {format}. Use 'pickle' or 'json'.")



if __name__=="__main__":
    contracts = ContractDataset(root="TSLA",
    total_start_date="20231001",
    total_end_date="20241001",
    contract_stride=30,
    interval_min=1,
    right="C",
    target_tte=45,
    tte_tolerance= (35, 55),
    moneyness="ITM",
    target_band=0.05,
    volatility_type= "period",
    volatility_scaled= True,
    volatility_scalar= 1.0,
    volatility_window= 0.8,)

    # contracts.save("contracts.pkl")

    # dataset = ContractDataset.load('contracts.pkl')

    # print(len(contracts))
    # print(dataset[1])




# class ForecastingDataset(Dataset):
#     """
#     A standard forecasting dataset class for PyTorch.

#     Args:
#         data (torch.Tensor): The time series data in a tensor of shape (num_channels, num_time_steps).
#         seq_len (int): The length of the input window.
#         pred_len (int): The length of the forecast window.
#         target_channels (list): The channels to forecast. If None, all channels are forecasted.
#         dtype (str): The datatype of the tensor.

#     __getitem__ method:

#         Args:
#             idx (int): The index of the input window in the time series.
#         Returns:
#             input_data (torch.Tensor): The lookback window of length seq_len for the given index.
#             target_data (torch.Tensor): The forecast window for the given index (continuation of input_data
#                                         shifted by pred_len)

#     """

#     def __init__(self, data, seq_len, pred_len, target_channels=None, dtype="float32"):

#         # Convert the data to a tensor and set the datatype
#         dtype = eval("torch." + dtype)
#         if not torch.is_tensor(data):
#             self.data = torch.from_numpy(data).type(dtype)
#         else:
#             self.data = data.type(dtype)
#         self.seq_len = seq_len
#         self.pred_len = pred_len

#         if target_channels:
#             self.target_channels = target_channels
#         else:
#             self.target_channels = list(range(self.data.shape[0]))

#     def __len__(self):
#         return self.data.shape[1] - self.seq_len - self.pred_len

#     def __getitem__(self, idx):
#         input_data = self.data[:, idx:idx+self.seq_len]
#         # target_data = self.data[:, idx+self.seq_len:idx+self.seq_len+self.pred_len]
#         target_data = self.data[self.target_channels, idx+self.seq_len:idx+self.seq_len+self.pred_len]
#         return input_data, target_data

# class UnivariateForecastingDataset(Dataset):
#     """
#     A standard forecasting dataset class for PyTorch.

#     Args:
#         data_x (torch.Tensor): The time series data in a tensor of shape (num_windows, seq_len).
#         data_y (torch.Tensor): The time series data in a tensor of shape (num_windows, pred_len).

#     __getitem__ method: Returns the input and target data for a given index, where the target window follows
#                         immediately after the input window in the time series.

#     """

#     def __init__(self, x, y, dtype="float32"):

#         # Convert the data to a tensor and set the datatype
#         dtype = eval("torch." + dtype)

#         if not torch.is_tensor(x):
#             self.x = torch.from_numpy(x).type(dtype)
#             self.y = torch.from_numpy(y).type(dtype)
#         else:
#             self.x = x.type(dtype)
#             self.y = y.type(dtype)

#             print(f"x: {self.x.shape}, y: {self.y.shape}")

#     def __len__(self):
#         return self.x.shape[0]

#     def __getitem__(self, idx):
#         return self.x[idx], self.y[idx]

# class ClassificationDataset(Dataset):
#     """
#     A classification dataset class for time series.

#     Args:
#         x (torch.Tensor): The input data in a tensor of shape (num_windows, seq_len).
#         y (torch.Tensor): The target data in a tensor of shape (num_windows).
#         ch_ids (torch.Tensor): The channel IDs in a tensor of shape (num_windows).
#         t (torch.Tensor): The time indices in a tensor of shape (num_windows).

#     """

#     def __init__(self, x, y, ch_ids=None, task="binary", full_channels=False):

#         # Data
#         self.x = x
#         self.y = torch.tensor(y) if isinstance(y, list) else y
#         ch_ids = torch.tensor(ch_ids) if isinstance(ch_ids, list) else ch_ids
#         self.ch_ids = ch_ids
#         self.full_channels = full_channels


#         # Parameters
#         self.task = task
#         self.len = x.size(0)
#         self.num_classes = len(torch.unique(self.y))

#         # Channel IDs
#         if not full_channels:
#             self.unique_ch_ids = torch.unique(ch_ids, sorted=True).tolist()
#             label_indices = torch.tensor([torch.where(ch_ids == unique_id)[0][0] for unique_id in self.unique_ch_ids])
#             self.unique_ch_labels = self.y[label_indices].tolist()
#             self.ch_labels = dict()
#             for i, ch_id in enumerate(self.unique_ch_ids):
#                 self.ch_labels[ch_id] = int(self.unique_ch_labels[i])

#         if ch_ids is not None and not full_channels:
#             unique_ch_ids, indices = torch.unique(ch_ids, sorted=True, return_inverse=True)

#             self.ch_id_list = unique_ch_ids.tolist()
#             self.num_channels = len(unique_ch_ids)
#             self.ch_targets = torch.zeros(self.num_channels)

#             # Get the unique labels for each channel
#             for i, ch_id in enumerate(unique_ch_ids):
#                 matching_indices = torch.where(ch_ids == ch_id)
#                 label = self.y[matching_indices][0]
#                 self.ch_targets[i] = label

#     def __len__(self):
#         return self.len

#     def __getitem__(self, idx):
#         """
#         In a dataloader it returns appropriate tensors for CrossEntropy loss.
#             x: (batch_size, 1, seq_len)
#             y: (batch_size,)
#             ch_ids: (batch_size,)
#         """

#         output = []

#         if self.task=="multi":
#             label = self.y[idx].long()
#         elif self.task=="binary":
#             label = self.y[idx].float()
#         else:
#             raise ValueError("Task must be either 'binary' or 'multi'.")

#         if self.ch_ids is not None and not self.full_channels:
#             output += [self.x[idx].unsqueeze(0), label, self.ch_ids[idx]]
#         else:
#             output += [self.x[idx].unsqueeze(0), label]

#         return tuple(output)
