import pickle
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
from typing import List, Tuple, Iterator, Dict, Any, Optional
from datetime import datetime, timedelta
from pydantic import BaseModel, Field
from rich.console import Console

from optrade.data.thetadata.stocks import get_stock_data
from optrade.data.thetadata.contracts import Contract
from optrade.src.preprocessing.data.volatility import get_historical_volatility

class ContractDataset:
    """
    A dataset containing options contracts generated with consistent parameters.

    Contracts are generated by starting from total_start_date and advancing by
    contract_stride days until reaching the last valid date that allows for
    contracts within the specified time-to-expiration tolerance.
    """

    def __init__(
        self,
        root: str = "AAPL",
        total_start_date: str = "20231107",
        total_end_date: str = "20241114",
        contract_stride: int = 5,
        interval_min: int = 1,
        right: str = "C",
        target_tte: int = 30,
        tte_tolerance: Tuple[int, int] = (25, 35),
        moneyness: str = "OTM",
        target_band: float = 0.05,
        volatility_type: str = "period",
        volatility_scaled: bool = True,
        volatility_scalar: float = 1.0,
        volatility_window: float = 0.8,
    ):
        """
        Initialize the ContractDataset with the specified parameters.

        Args:
            root: The security root symbol
            total_start_date: Start date for the dataset (YYYYMMDD)
            total_end_date: End date for the dataset (YYYYMMDD)
            contract_stride: Days between consecutive contracts
            interval_min: Data interval in minutes
            right: Option type (C/P)
            target_tte: Target time to expiration in days
            tte_tolerance: Acceptable range for TTE as (min_days, max_days)
            moneyness: Contract moneyness (OTM/ATM/ITM)
            target_band: Target percentage band for strike selection
            volatility_type: Type of volatility measure to use
            volatility_scaled: Whether to scale by volatility
            volatility_scalar: Scaling factor for volatility
            volatility_window: Window size for volatility calculation
        """
        self.root = root
        self.total_start_date = total_start_date
        self.total_end_date = total_end_date
        self.contract_stride = contract_stride
        self.interval_min = interval_min
        self.right = right
        self.target_tte = target_tte
        self.tte_tolerance = tte_tolerance
        self.moneyness = moneyness
        self.target_band = target_band
        self.volatility_type = volatility_type
        self.volatility_scaled = volatility_scaled
        self.volatility_scalar = volatility_scalar
        self.volatility_window = volatility_window

        self.ctx = Console()
        self.contracts = []

    def get_hist_vol(self):
        # Calculate number of days to use for historical volatility
        total_days = (pd.to_datetime(self.total_end_date, format='%Y%m%d') - pd.to_datetime(self.total_start_date, format='%Y%m%d')).days
        num_vol_days = int(self.volatility_window * total_days)
        vol_end_date = (pd.to_datetime(self.total_start_date, format='%Y%m%d') + pd.Timedelta(days=num_vol_days)).strftime('%Y%m%d')

        stock_data = get_stock_data(
            root=self.root,
            start_date=self.total_start_date,
            end_date=self.total_end_date,
            interval_min=self.interval_min,
            clean_up=True,
        )

        # Select only the first num_vol_days for calculating volatility
        stock_data = stock_data.loc[stock_data['datetime'] <= vol_end_date]

        # Calculate historical volatility
        self.hist_vol = get_historical_volatility(stock_data, self.volatility_type)
        self.ctx.log(f"Historical volatility from {self.total_start_date} to {vol_end_date}: {self.hist_vol}")

    def generate_contracts(self):
        """
        Generate all contracts in the dataset based on configuration parameters.
        """
        # Parse dates
        start_date = datetime.strptime(self.total_start_date, "%Y%m%d")
        end_date = datetime.strptime(self.total_end_date, "%Y%m%d")
        max_tte = max(self.tte_tolerance)

        # Calculate the latest possible start date
        latest_start = end_date - timedelta(days=max_tte)

        # Generate contracts
        current_date = start_date
        self.ctx.log(f"Current start date: {current_date}")

        while current_date <= latest_start:
            # Format initial date string
            date_str = current_date.strftime("%Y%m%d")
            self.ctx.log(f"Finding optimal contract for {date_str}")
            attempt_date = current_date
            contract = None

            # Find a valid contract for the current date. Some dates may be ineligible due to holidays or weekends.
            while contract is None and attempt_date <= latest_start:
                attempt_date_str = attempt_date.strftime("%Y%m%d")
                try:
                    contract = Contract.find_optimal(
                        root=self.root,
                        start_date=attempt_date_str,
                        interval_min=self.interval_min,
                        right=self.right,
                        target_tte=self.target_tte,
                        tte_tolerance=self.tte_tolerance,
                        moneyness=self.moneyness,
                        target_band=self.target_band,
                        hist_vol=self.hist_vol,
                        volatility_scaled=self.volatility_scaled,
                        volatility_scalar=self.volatility_scalar,
                    )

                    if attempt_date > current_date:
                        self.ctx.log(f"Found valid contract at shifted date: {attempt_date_str}")

                except Exception as e:
                    self.ctx.log(f"Failed to find contract for {attempt_date_str}: {str(e)}")
                    attempt_date += timedelta(days=1)

                    # Check if we've run out of valid dates
                    if attempt_date > latest_start:
                        self.ctx.log(f"Unable to find valid contract starting from {date_str}")
                        break

                    continue

            # If we found a valid contract, add it and advance by stride
            if contract is not None:
                self.contracts.append(contract)
                self.ctx.log(f"Added contract: {contract}")
                current_date = attempt_date + timedelta(days=self.contract_stride)
            else:
                # If no contract was found, advance by one day to try the next period
                current_date += timedelta(days=1)

            self.ctx.log(f"Next start date: {current_date}")

    def __len__(self) -> int:
        """Get the number of contracts in the dataset."""
        return len(self.contracts)

    def __getitem__(self, idx):
        """Get a contract by index."""
        return self.contracts[idx]

    def __iter__(self) -> Iterator:
        """Iterate through contracts."""
        return iter(self.contracts)

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the dataset to a dictionary suitable for serialization.
        Separates initialization parameters from computed attributes.
        """
        return {
            "params": {
                "root": self.root,
                "total_start_date": self.total_start_date,
                "total_end_date": self.total_end_date,
                "contract_stride": self.contract_stride,
                "interval_min": self.interval_min,
                "right": self.right,
                "target_tte": self.target_tte,
                "tte_tolerance": self.tte_tolerance,
                "moneyness": self.moneyness,
                "target_band": self.target_band,
                "volatility_type": self.volatility_type,
                "volatility_scaled": self.volatility_scaled,
                "volatility_scalar": self.volatility_scalar,
                "volatility_window": self.volatility_window,
            },
            "computed": {
                "hist_vol": self.hist_vol
            },
            "contracts": [contract.model_dump() for contract in self.contracts]
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ContractDataset":
        """
        Create a new dataset instance from a dictionary.
        Handles both initialization parameters and computed attributes.
        """
        # Create instance with initialization parameters
        instance = cls(**data["params"])

        # Restore computed attributes
        instance.hist_vol = data["computed"]["hist_vol"]

        # Restore contracts
        instance.contracts = [Contract(**contract_dict) for contract_dict in data["contracts"]]

        return instance

    def save(self, filepath: Optional[str] = None) -> str:
        """
        Save the dataset to a pickle file.

        Args:
            filepath: Optional custom filepath. If None, generates default name

        Returns:
            str: Path where the pickle file was saved
        """
        if filepath is None:
            # Generate default filename using dataset parameters
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"{self.root}_contracts_{self.total_start_date}_{self.total_end_date}_{timestamp}.pkl"
            filepath = Path("contract_data") / filename

        # Ensure directory exists
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)

        # Convert to dictionary and save
        data = self.to_dict()
        with open(filepath, 'wb') as f:
            pickle.dump(data, f)

        self.ctx.log(f"Dataset saved to {filepath}")
        return str(filepath)

    @classmethod
    def load(cls, filepath: str) -> "ContractDataset":
        """
        Load a dataset from a pickle file.

        Args:
            filepath: Path to the pickle file

        Returns:
            ContractDataset: Reconstructed dataset with all contracts
        """
        with open(filepath, 'rb') as f:
            data = pickle.load(f)

        instance = cls.from_dict(data)
        instance.ctx.log(f"Dataset loaded from {filepath}")
        return instance

class ForecastingDataset(Dataset):
    """

    Args:
        data (torch.Tensor): Tensor of shape (num_time_steps, num_features)
        seq_len (int): Length of the lookback window
        pred_len (int): Length of the forecast window
        target_channels (list): Channels to forecast. By default target_channels=[0], which corresponds to the
                                option midprice returns. If None, all channels will be returned for the target.
        dtype (str): Desired data type of the tensor.
    """

    def __init__(self, data, seq_len, pred_len, target_channels=[0], dtype="float32"):
        dtype = eval("torch." + dtype)
        if not torch.is_tensor(data):
            self.inputs = torch.from_numpy(data).type(dtype)
        else:
            self.inputs = data.type(dtype)
        self.seq_len = seq_len
        self.pred_len = pred_len

        if target_channels != []:
            self.target_channels = target_channels

    def __len__(self):
        return self.data.shape[0] - self.seq_len - self.pred_len

    def __getitem__(self, idx):
        input = self.data[idx:idx+self.seq_len]

        if self.target_channels:
            target = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len][self.target_channels]
        else:
            target = self.data[idx+self.seq_len:idx+self.seq_len+self.pred_len]

        return input, target


class IntradayForecastingDataset(Dataset):
    """
    A standard forecasting dataset class for PyTorch.

    Args:
        data (torch.Tensor): The time series data in a tensor of shape (num_channels, num_time_steps).
        seq_len (int): The length of the input window.
        pred_len (int): The length of the forecast window.
        target_channels (list): The channels to forecast. If None, all channels are forecasted.
        dtype (str): The datatype of the tensor.

    __getitem__ method:

        Args:
            idx (int): The index of the input window in the time series.
        Returns:
            input_data (torch.Tensor): The lookback window of length seq_len for the given index.
            target_data (torch.Tensor): The forecast window for the given index (continuation of input_data
                                        shifted by pred_len)

    """

    def __init__(self, inputs, targets, dtype="float32"):

        # Convert the data to a tensor and set the datatype
        dtype = eval("torch." + dtype)
        if not torch.is_tensor(inputs):
            self.inputs = torch.from_numpy(inputs).type(dtype)
            self.targets = torch.from_numpy(targets).type(dtype)
        else:
            self.inputs = inputs.type(dtype)
            self.targets = targets.type(dtype)

    def __len__(self):
        return self.inputs.shape[0]

    def __getitem__(self, idx):
        return self.input[idx], self.target[idx]


if __name__=="__main__":
    contracts = ContractDataset(root="AMZN",
    total_start_date="20220101",
    total_end_date="20220301",
    contract_stride=15,
    interval_min=1,
    right="P",
    target_tte=3,
    tte_tolerance= (1, 10),
    moneyness="ITM",
    target_band=0.05,
    volatility_type= "period",
    volatility_scaled= True,
    volatility_scalar= 1.0,
    volatility_window= 0.8,)

    # Generate contracts
    contracts.get_hist_vol()
    contracts.generate_contracts()

    # Save the contracts
    contracts.save("contracts.pkl")

    # Load the generated contracts into a fresh dataset
    dataset = ContractDataset.load("contracts.pkl")


    # Print out attributes
    print(f"Root: {contracts.root}")
    print(f"Total start date: {contracts.total_start_date}")
    print(f"Total end date: {contracts.total_end_date}")
    print(f"Contract stride: {contracts.contract_stride}")
    print(f"Interval min: {contracts.interval_min}")
    print(f"Right: {contracts.right}")
    print(f"Target TTE: {contracts.target_tte}")
    print(f"TTE tolerance: {contracts.tte_tolerance}")
    print(f"Moneyness: {contracts.moneyness}")
    print(f"Target band: {contracts.target_band}")
    print(f"Volatility type: {contracts.volatility_type}")
    print(f"Volatility scaled: {contracts.volatility_scaled}")
    print(f"Volatility scalar: {contracts.volatility_scalar}")
    print(f"Volatility window: {contracts.volatility_window}")
    print(f"Number of contracts: {len(contracts)}")
    print(f"First contract: {contracts[0]}")

    # contracts.save("contracts.pkl")

    # dataset = ContractDataset.load('contracts.pkl')

    # print(len(contracts))
    # print(dataset[1])




# class ForecastingDataset(Dataset):
#     """
#     A standard forecasting dataset class for PyTorch.

#     Args:
#         data (torch.Tensor): The time series data in a tensor of shape (num_channels, num_time_steps).
#         seq_len (int): The length of the input window.
#         pred_len (int): The length of the forecast window.
#         target_channels (list): The channels to forecast. If None, all channels are forecasted.
#         dtype (str): The datatype of the tensor.

#     __getitem__ method:

#         Args:
#             idx (int): The index of the input window in the time series.
#         Returns:
#             input_data (torch.Tensor): The lookback window of length seq_len for the given index.
#             target_data (torch.Tensor): The forecast window for the given index (continuation of input_data
#                                         shifted by pred_len)

#     """

#     def __init__(self, data, seq_len, pred_len, target_channels=None, dtype="float32"):

#         # Convert the data to a tensor and set the datatype
#         dtype = eval("torch." + dtype)
#         if not torch.is_tensor(data):
#             self.data = torch.from_numpy(data).type(dtype)
#         else:
#             self.data = data.type(dtype)
#         self.seq_len = seq_len
#         self.pred_len = pred_len

#         if target_channels:
#             self.target_channels = target_channels
#         else:
#             self.target_channels = list(range(self.data.shape[0]))

#     def __len__(self):
#         return self.data.shape[1] - self.seq_len - self.pred_len

#     def __getitem__(self, idx):
#         input_data = self.data[:, idx:idx+self.seq_len]
#         # target_data = self.data[:, idx+self.seq_len:idx+self.seq_len+self.pred_len]
#         target_data = self.data[self.target_channels, idx+self.seq_len:idx+self.seq_len+self.pred_len]
#         return input_data, target_data

# class UnivariateForecastingDataset(Dataset):
#     """
#     A standard forecasting dataset class for PyTorch.

#     Args:
#         data_x (torch.Tensor): The time series data in a tensor of shape (num_windows, seq_len).
#         data_y (torch.Tensor): The time series data in a tensor of shape (num_windows, pred_len).

#     __getitem__ method: Returns the input and target data for a given index, where the target window follows
#                         immediately after the input window in the time series.

#     """

#     def __init__(self, x, y, dtype="float32"):

#         # Convert the data to a tensor and set the datatype
#         dtype = eval("torch." + dtype)

#         if not torch.is_tensor(x):
#             self.x = torch.from_numpy(x).type(dtype)
#             self.y = torch.from_numpy(y).type(dtype)
#         else:
#             self.x = x.type(dtype)
#             self.y = y.type(dtype)

#             print(f"x: {self.x.shape}, y: {self.y.shape}")

#     def __len__(self):
#         return self.x.shape[0]

#     def __getitem__(self, idx):
#         return self.x[idx], self.y[idx]

# class ClassificationDataset(Dataset):
#     """
#     A classification dataset class for time series.

#     Args:
#         x (torch.Tensor): The input data in a tensor of shape (num_windows, seq_len).
#         y (torch.Tensor): The target data in a tensor of shape (num_windows).
#         ch_ids (torch.Tensor): The channel IDs in a tensor of shape (num_windows).
#         t (torch.Tensor): The time indices in a tensor of shape (num_windows).

#     """

#     def __init__(self, x, y, ch_ids=None, task="binary", full_channels=False):

#         # Data
#         self.x = x
#         self.y = torch.tensor(y) if isinstance(y, list) else y
#         ch_ids = torch.tensor(ch_ids) if isinstance(ch_ids, list) else ch_ids
#         self.ch_ids = ch_ids
#         self.full_channels = full_channels


#         # Parameters
#         self.task = task
#         self.len = x.size(0)
#         self.num_classes = len(torch.unique(self.y))

#         # Channel IDs
#         if not full_channels:
#             self.unique_ch_ids = torch.unique(ch_ids, sorted=True).tolist()
#             label_indices = torch.tensor([torch.where(ch_ids == unique_id)[0][0] for unique_id in self.unique_ch_ids])
#             self.unique_ch_labels = self.y[label_indices].tolist()
#             self.ch_labels = dict()
#             for i, ch_id in enumerate(self.unique_ch_ids):
#                 self.ch_labels[ch_id] = int(self.unique_ch_labels[i])

#         if ch_ids is not None and not full_channels:
#             unique_ch_ids, indices = torch.unique(ch_ids, sorted=True, return_inverse=True)

#             self.ch_id_list = unique_ch_ids.tolist()
#             self.num_channels = len(unique_ch_ids)
#             self.ch_targets = torch.zeros(self.num_channels)

#             # Get the unique labels for each channel
#             for i, ch_id in enumerate(unique_ch_ids):
#                 matching_indices = torch.where(ch_ids == ch_id)
#                 label = self.y[matching_indices][0]
#                 self.ch_targets[i] = label

#     def __len__(self):
#         return self.len

#     def __getitem__(self, idx):
#         """
#         In a dataloader it returns appropriate tensors for CrossEntropy loss.
#             x: (batch_size, 1, seq_len)
#             y: (batch_size,)
#             ch_ids: (batch_size,)
#         """

#         output = []

#         if self.task=="multi":
#             label = self.y[idx].long()
#         elif self.task=="binary":
#             label = self.y[idx].float()
#         else:
#             raise ValueError("Task must be either 'binary' or 'multi'.")

#         if self.ch_ids is not None and not self.full_channels:
#             output += [self.x[idx].unsqueeze(0), label, self.ch_ids[idx]]
#         else:
#             output += [self.x[idx].unsqueeze(0), label]

#         return tuple(output)
